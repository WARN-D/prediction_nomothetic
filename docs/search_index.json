[["introduction.html", "Integrating ecological momentary and passive sensing data to improve depression severity prediction: insights from the WARN-D study 1 Introduction", " Integrating ecological momentary and passive sensing data to improve depression severity prediction: insights from the WARN-D study Rayyan Tutunji 2025-11-04 1 Introduction This is the notebook accompanying the preprint/publication Integrating ecological momentary and passive sensing data to improve depression severity prediction: insights from the WARN-D study. The notebook contains four main sections. Parts 2 and 4 are coded in R, with part 3 run in python. In 2, the data preperation and processing is carried out. The descriptive statistics that are used in the paper are also done in this part. Between section 2 and 3, code is run on the HPC ALICE cluster of Leiden Univeristy. This process is not integrated into the notebook, but is provided as a code in the repository. The results of the model calibration and cross-validation are done using the code under section 3 in python. Here we also generate the SHAP values. All this is saved into .csv files that we use for the end results/figures which are presented in section 4, in R again. "],["data-prep.html", "2 Data Prep 2.1 Processing 2.2 Descriptives 2.3 Imputation 2.4 Dataset Creation 2.5 Dataset Export", " 2 Data Prep Two data sources are used in the current analysis. Stage 1 Qualtrics questionnaires with baseline surveys, Stage 2 EMA and Garmin data spanning 85 days. Stage 3 Qualtrics questionnaires with follow-up surveys are not used. Data was processed via (WARN-Pipe)[https://github.com/WARN-D/WARNPipe], a set of functions written to process data acquired in WARN-D. The exact steps implemented to process the data can be found in the scripts directory of the project specific GitHub directory, and a brief summary is provided here. Stage 1 data was analyzed as follows: Variable names were recoded to match the codebook in cases where data exports were inconsistent NAN correction was applied to instances where -998 and -999 were present, making NANs explicit Scoring was applied to the different scales in the data Data was cleaned to remove potential identifiers (I.P. address, emails, etc…) Stage 2 data was processed as follows: First, the different ethica exports containing various stages of EMA data were merged As Ethica stores data with one question per row, we also widen the data to a “regular” long EMA format with one survey per row instead We finally expand the data to include all potential prompts, as missed prompts were not part of the export Garmin data was synced to the EMA data in windows of 15, 30, 60, 120, and 240 minutes before each survey. This was done on the ALICE cluster as well. N.B.: File paths are stored as Renviron variables and are not included in the commit. # Set dirs for files we need df_base = fread(here(datasets, &#39;WARND_stage1_c1234_fullset_v2025_10_24.csv&#39;)) df_ema = fread(here(datasets, &#39;WARND_stage2_c1234_fullset_before_60min_v2025_10_24.csv&#39;)) 2.1 Processing Now that the data is loaded, we can proceed to transform and generate some variables out of the individual items. This is mostly necessary for the EMA data, were we can estimate some features both for the daily variables, and the weekly variables. We first define our variables of interest, and make vectors out of them, and then apply several transformation on them as described in the next sections. We estimate two sets of features: Daily and Weekly. Daily: 4x/day: Here we do some simple processing of the data. First we get vectors of all potential features we’re interested in from both the ethica and garmin data. We estimate sleep starts and end times in minutes, and then we fill in categorical items. In cases where they were not endorsed or selected, we get NAs, which is not correct. So we fill in these instances with 0’s Morning/Evening (1x/day) We do the same as above for the categorical items We forward fill the morning items within the same day We back fill the evening items within the same day Evening (1x/day) Weekly: We make sum scores for the depression questionnaires Using the sum scores, we set a cut-off for moderate depression 2.1.1 Daily Features # Make vectors out of variables we want ema_day_vars = c( &quot;overwhelm_d&quot;, &quot;motivated_d&quot;, &quot;stressed_d&quot;, &quot;ruminate_d&quot;, &quot;tired_d&quot;, &quot;nervous_d&quot;, &quot;sad_d&quot;, &quot;relaxed_d&quot;, &quot;cheerful_d&quot;, &quot;irritable_d&quot;, &quot;activity_enjoy_d&quot;, &quot;location_d&quot;,&quot;offline_enjoy_d&quot;, &quot;online_enjoy_d&quot;) # Also lets do same for EPA epa_day_hr = colnames(df_ema %&gt;% select(starts_with(&quot;hr_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_w&quot;)) ) epa_day_stress = colnames(df_ema %&gt;% select(starts_with(&quot;stress_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_w&quot;)) ) epa_day_act = colnames(df_ema %&gt;% select(starts_with(&quot;intraday_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_w&quot;)) ) epa_day_bat = colnames(df_ema %&gt;% select(starts_with(&quot;body_bat_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_w&quot;)) ) epa_sleep = colnames(df_ema %&gt;% select(starts_with(&quot;sleep_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_m&quot;)) %&gt;% select(!starts_with(&quot;sleep_hr&quot;)) ) epa_sleep_hr = colnames(df_ema %&gt;% select(starts_with(&quot;sleep_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_m&quot;)) %&gt;% select(starts_with(&quot;sleep_hr&quot;)) ) epa_day_summary = colnames(df_ema %&gt;% select(starts_with(&quot;day_&quot;)) %&gt;% select(!ends_with(&#39;samp&#39;)) %&gt;% select(!ends_with(&quot;_num&quot;)) ) # put them all togethet epa_day_vars = c(epa_day_hr, epa_day_stress, epa_day_act[c(2:5,7,8)], epa_day_bat) epa_sleep_vars = c(epa_sleep_hr, &#39;sleep_start&#39;, &#39;sleep_end&#39;, &#39;sleep_dur_tot_s&#39;) # Convert sleep start and end times to minutes from midnight df_ema = df_ema %&gt;% dplyr::mutate( across(all_of(c(&#39;sleep_start&#39;, &#39;sleep_end&#39;)), ~ { hour_part &lt;- hour(.x) minute_part &lt;- minute(.x) minutes_from_midnight &lt;- hour_part * 60 + minute_part minutes_from_midnight}, .names = &quot;{.col}&quot; ) ) # Now fix the daily category items ## First select the coloumns activity_cat_d = colnames(df_ema)[grep(&#39;^activity_cat_d&#39;, colnames(df_ema))] online_cat_d = colnames(df_ema)[grep(&#39;^online_cat_d&#39;, colnames(df_ema))] offline_cat_d = colnames(df_ema)[grep(&#39;^offline_cat_d&#39;, colnames(df_ema))] categorical_items_d = c(activity_cat_d, offline_cat_d, online_cat_d) df_ema = df_ema %&gt;% mutate(across(all_of(categorical_items_d), ~ if_else(.x &gt;0, 1, .x)), # First turn into binary items for categories across(all_of(categorical_items_d), ~ if_else( (is.na(.x) &amp; !is.na(activity_enjoy_d)), 0, .x)) )# Now replace the unselected MCQ options with 0 2.1.2 Morning/Evening Features ema_mor_vars = c(&#39;sleep_qual_m&#39;,&#39;sleep_rest_m&#39;,&#39;outlook_m&#39;) pos_exp_cats = colnames(df_ema)[grep(&#39;^pos_exp_cat_e&#39;, colnames(df_ema))] neg_exp_cats = colnames(df_ema)[grep(&#39;^neg_exp_cat_e&#39;, colnames(df_ema))] substance_cats = colnames(df_ema)[grep(&#39;^substance_e&#39;, colnames(df_ema))] ema_eve_vars = c( &#39;discomfort_e&#39;, &#39;emo_reg_e&#39;, &#39;useful_e&#39;, &#39;neg_exp_e&#39;, &#39;pos_exp_e&#39;, pos_exp_cats, neg_exp_cats, substance_cats) ema_eve_vars = ema_eve_vars[!grepl(&quot;_NA&quot;, ema_eve_vars)] df_ema = df_ema %&gt;% group_by(external_id, date) %&gt;% # Here we check for the MC questions, if an evening is anwered by cat is NA, replace with 0 mutate(across(all_of(c(pos_exp_cats, neg_exp_cats, substance_cats)), ~ if_else( (is.na(.x) &amp; !is.na(emo_reg_e)), 0, .x))) %&gt;% fill(all_of(ema_mor_vars), .direction = &#39;down&#39;) %&gt;% fill(all_of(ema_eve_vars), .direction = &#39;up&#39;) %&gt;% ungroup() %&gt;% mutate(across(all_of(c(neg_exp_cats, pos_exp_cats, substance_cats)), ~ if_else(.x&gt;0, 1, .x))) 2.1.3 Weekly Features # First make the grouping factor df_ema$week_n = isoweek(df_ema$date) # create a list of PHQ items phq_items_w = c(&quot;phq5_appet_loss_w&quot;, &quot;phq7_concentr_w&quot; , &quot;phq5_overeat_w&quot;, &quot;phq8_retar_w&quot;, &quot;phq9_suicide_w&quot;, &quot;phq12_libido_w&quot;, &quot;phq6_worthless_w&quot; , &quot;phq11_irrit_w&quot;, &quot;phq2_sad_w&quot; , &quot;phq8_agitate_w&quot;, &quot;phq2_hopeless_w&quot;, &quot;phq1_anhedo_w&quot; , &quot;phq3_insomnia_w&quot;, &quot;phq3_hypersom_w&quot; , &quot;phq4_tired_w&quot;) # First we estimate the evening depression df_ema = df_ema %&gt;% mutate(phq2_sum_e = anhedonia_e + depressed_e) # estimate PHQ9 Sum score for weekly too df_ema = df_ema %&gt;% dplyr::mutate(phq9_sum_w = pseudo_phq9_sum_w) %&gt;% ungroup() # Create a week variable for grouping df_ema = df_ema %&gt;% # Estimate the week number and weekly days group_by(external_id) %&gt;% mutate(week_nr = floor(as.numeric(difftime(date, min(date, na.rm = T), units = &quot;weeks&quot;))) ) %&gt;% group_by(external_id, week_nr) %&gt;% mutate(day_nr = as.numeric(difftime(date, min(date, na.rm = T), units = &quot;days&quot;))) %&gt;% ungroup() # Now estimate the daily and weekly depression df_ema = df_ema %&gt;% # Estimate the centered valyes for the phq9 dplyr::group_by(external_id) %&gt;% dplyr::mutate(across(all_of(c(phq_items_w, &#39;phq9_sum_w&#39;)), ~ (.x - mean(.x, na.rm =T)),.names = &quot;{.col}_c&quot; )) %&gt;% arrange(date) %&gt;% # Fill backwards daily group_by(external_id, day_nr) %&gt;% fill(phq2_sum_e, .direction = &#39;up&#39;) %&gt;% ungroup() %&gt;% # Fill backwards weekly group_by(external_id, week_nr) %&gt;% fill(phq9_sum_w, .direction = &#39;updown&#39;) %&gt;% ungroup() # Estimate the cutoff for PHQ 9 df_ema$phq9_cuttoff = ifelse(df_ema$phq9_sum_w &gt; 9, 1, 0) 2.1.4 Compliance Estimates Now we make some compliance estimates for the data. We do this for the EMA and garmin data separately. # Add comploance rates (without prompt_5) df_ema_compliance = df_ema %&gt;% filter(prompt_num != 5) df_ema_compliance$compliance = esmpack::calc.nomiss(sad_d, id = external_id, data = df_ema_compliance, prop = T, expand = T) df_ema_compliance$compliance_hr = esmpack::calc.nomiss(hr_mean, id = external_id, data = df_ema_compliance, prop = T, expand = T) df_ema_compliance$compliance_stress = esmpack::calc.nomiss(stress_mean, id = external_id, data = df_ema_compliance, prop = T, expand = T) df_ema_compliance = df_ema_compliance %&gt;% select(external_id, starts_with(&quot;compliance&quot;)) %&gt;% distinct() # Add compliance rates to main df df_ema = merge(df_ema, df_ema_compliance, by = &#39;external_id&#39;, all = T) 2.1.5 Data Merger Now that the preprocessing is done, we attach the EMA data to the baseline data so that we can get per-subject values for our descriptives. # Merge base to ema df_merge = merge(df_base, df_ema, by = c(&#39;external_id&#39;, &#39;cohort&#39;), all = T) # Filter based on compliance df_merge = df_merge[df_merge$compliance &gt; 0.3, ] df_merge = df_merge[!is.na(df_merge$cohort), ] df_merge = df_merge[!is.na(df_merge$prompt_num), ] # read training list and filter subjects training_set = read.csv(paste(project_folder, &#39;data&#39;, &#39;participants&#39;, &#39;linked_ids_training_set_seed666_by_Cohort.csv&#39;, sep = &quot;/&quot;)) 2.2 Descriptives Lets take a look at some general data exploration and descriptives here. I specifically want to look at demopgraphics and EMA compliance rates. I’ll also take a look at the general distributions and trends in the data. We check the correlations between our different features. We use the baseline PHQ, Garmin, and EMA variables. We first check the overall correlations, and then check the ones with r &gt; 0.7 for feature selection. This helps us make our models a bit more parsimonious. Based on these matrices, we see that HR mean is highly correlated with the mean, and minimum of the stress measure, mean of the body battery is also highly correlated with the mean and minimum. 2.2.1 Demographics library(summarytools) df_descriptive = merge(training_set, df_merge, by = c(&quot;external_id&quot;, &quot;cohort&quot;), all.y = T) df_descriptive &lt;- df_descriptive%&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(sum_lidas = sum(core1_lidas, core2_lidas, core3_lidas, lidas_13,appetite_lidas, sleep_lidas,motor_lidas,concentr_lidas,lidas_27,lidas_28, na.rm=T), lidas_cut_off = ifelse( ((core1_lidas==1 | core2_lidas == 1 | core3_lidas == 1) &amp; sum_lidas&gt;=5 &amp; lidas_29==1), 1, 0)) df_desc_summ = df_descriptive %&gt;% mutate(training_set = if_else(training_set == 1, &#39;train&#39;, &#39;test&#39;)) %&gt;% group_by(training_set) %&gt;% distinct(external_id, .keep_all = T) %&gt;% select(age, sex, lvl_edu, nationality, sss, pseudo_phq9_sum, lidas_cut_off) %&gt;% mutate(across(c(&quot;sex&quot;, &quot;lvl_edu&quot;, &quot;nationality&quot;, &quot;lidas_cut_off&quot;), ~ factor(.x))) ## Adding missing grouping variables: `training_set` dfSummary(df_desc_summ, plain.ascii = T, graph.col = F) ## Data Frame Summary ## df_desc_summ ## Group: training_set = train ## Dimensions: 855 x 8 ## Duplicates: 57 ## ## ----------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Valid Missing ## ---- ----------------- ------------------------ -------------------- ---------- --------- ## 2 age Mean (sd) : 22.7 (4.1) 28 distinct values 854 1 ## [integer] min &lt; med &lt; max: (99.9%) (0.1%) ## 18 &lt; 22 &lt; 61 ## IQR (CV) : 4 (0.2) ## ## 3 sex 1. 0 124 (14.5%) 854 1 ## [factor] 2. 1 730 (85.5%) (99.9%) (0.1%) ## ## 4 lvl_edu 1. 3 418 (49.3%) 848 7 ## [factor] 2. 4 35 ( 4.1%) (99.2%) (0.8%) ## 3. 5 47 ( 5.5%) ## 4. 6 3 ( 0.4%) ## 5. 7 277 (32.7%) ## 6. 8 66 ( 7.8%) ## 7. 9 2 ( 0.2%) ## ## 5 nationality 1. 1 453 (53.0%) 854 1 ## [factor] 2. 2 338 (39.6%) (99.9%) (0.1%) ## 3. 3 63 ( 7.4%) ## ## 6 sss Mean (sd) : 7 (1.4) 1 : 1 ( 0.1%) 854 1 ## [integer] min &lt; med &lt; max: 2 : 2 ( 0.2%) (99.9%) (0.1%) ## 1 &lt; 7 &lt; 10 3 : 9 ( 1.1%) ## IQR (CV) : 2 (0.2) 4 : 40 ( 4.7%) ## 5 : 65 ( 7.6%) ## 6 : 135 (15.8%) ## 7 : 292 (34.2%) ## 8 : 225 (26.3%) ## 9 : 73 ( 8.5%) ## 10 : 12 ( 1.4%) ## ## 7 pseudo_phq9_sum Mean (sd) : 7.2 (4.5) 25 distinct values 854 1 ## [integer] min &lt; med &lt; max: (99.9%) (0.1%) ## 0 &lt; 7 &lt; 25 ## IQR (CV) : 6 (0.6) ## ## 8 lidas_cut_off 1. 0 495 (57.9%) 855 0 ## [factor] 2. 1 360 (42.1%) (100.0%) (0.0%) ## ----------------------------------------------------------------------------------------- ## ## Group: training_set = NA ## Dimensions: 358 x 8 ## Duplicates: 9 ## ## ----------------------------------------------------------------------------------------- ## No Variable Stats / Values Freqs (% of Valid) Valid Missing ## ---- ----------------- ------------------------ -------------------- ---------- --------- ## 2 age Mean (sd) : 22.4 (3.4) 20 distinct values 358 0 ## [integer] min &lt; med &lt; max: (100.0%) (0.0%) ## 18 &lt; 22 &lt; 42 ## IQR (CV) : 4 (0.2) ## ## 3 sex 1. 0 58 (16.2%) 358 0 ## [factor] 2. 1 300 (83.8%) (100.0%) (0.0%) ## ## 4 lvl_edu 1. 3 165 (46.6%) 354 4 ## [factor] 2. 4 14 ( 4.0%) (98.9%) (1.1%) ## 3. 5 21 ( 5.9%) ## 4. 6 1 ( 0.3%) ## 5. 7 128 (36.2%) ## 6. 8 24 ( 6.8%) ## 7. 9 1 ( 0.3%) ## ## 5 nationality 1. 1 184 (51.4%) 358 0 ## [factor] 2. 2 137 (38.3%) (100.0%) (0.0%) ## 3. 3 37 (10.3%) ## ## 6 sss Mean (sd) : 6.8 (1.4) 2 : 1 ( 0.3%) 358 0 ## [integer] min &lt; med &lt; max: 3 : 6 ( 1.7%) (100.0%) (0.0%) ## 2 &lt; 7 &lt; 10 4 : 18 ( 5.0%) ## IQR (CV) : 2 (0.2) 5 : 32 ( 8.9%) ## 6 : 57 (15.9%) ## 7 : 127 (35.5%) ## 8 : 92 (25.7%) ## 9 : 21 ( 5.9%) ## 10 : 4 ( 1.1%) ## ## 7 pseudo_phq9_sum Mean (sd) : 7.5 (4.7) 21 distinct values 358 0 ## [integer] min &lt; med &lt; max: (100.0%) (0.0%) ## 0 &lt; 7 &lt; 22 ## IQR (CV) : 7 (0.6) ## ## 8 lidas_cut_off 1. 0 211 (58.9%) 358 0 ## [factor] 2. 1 147 (41.1%) (100.0%) (0.0%) ## ----------------------------------------------------------------------------------------- 2.2.2 Compliance Rates Here are the compliance rates message(&quot;EMA Compliance&quot;) ## EMA Compliance (summary(df_ema_compliance$compliance, title=&quot;EMA&quot;)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.2703 0.5382 0.5097 0.7500 0.9941 message(&quot;Garmin Compliance&quot;) ## Garmin Compliance summary(df_ema_compliance$compliance_hr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.4647 0.8235 0.6704 0.9265 1.0000 And now plots for the distribution of the compliance rates: df_merge$cohort = as.factor(df_merge$cohort) ggpubr::ggarrange( ggplot(df_merge, aes(x=compliance, fill = cohort)) + geom_histogram(bins = 50) + labs(x=&quot;Compliance-EMA&quot;) + facet_grid(.~cohort), ggplot(df_merge, aes(x=compliance_hr, fill = cohort)) + geom_histogram(bins = 50) + labs(x=&quot;Compliance-HR&quot;) + facet_grid(.~cohort), ggplot(df_merge, aes(x=compliance_stress, fill = cohort)) + geom_histogram(bins=50) + labs(x=&quot;Compliance-Stress&quot;)+ facet_grid(.~cohort), nrow = 3, common.legend = TRUE, legend = &quot;bottom&quot;) ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database ## Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font family &#39;SegoeIcons&#39; not found in PostScript font database 2.2.3 Feature Correlations We take a look at the correlation of features, and then also check which are the features with super high correlations. 2.2.3.1 Full Features df_merge_corr = df_merge %&gt;% select(external_id, day_num, prompt_num, pseudo_phq9_sum, phq2_sum_e, phq9_sum_w, all_of(ema_day_vars), all_of(epa_day_vars),starts_with(&quot;sleep_dur&quot;), starts_with(&#39;sleep_hr&#39;) ) %&gt;% arrange(external_id, day_num, prompt_num) corr_all = correlate(df_merge_corr, diagonal = T, method = &#39;spearman&#39; ) ## Non-numeric variables removed from input: `external_id` ## Correlation computed with ## • Method: &#39;spearman&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; corr_all_plot = corrplot(as_matrix(corr_all), type = &#39;lower&#39;,method = &#39;color&#39;, addgrid.col = &#39;aliceblue&#39;, tl.cex = 0.5) 2.2.3.2 Highly Correlated Features corr_high = correlate(df_merge_corr, diagonal = T) ## Non-numeric variables removed from input: `external_id` ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; corr_high = corr_high %&gt;% mutate( across(everything(), ~ ifelse((.x &lt;= 0.7 &amp; .x &gt;= -0.7), 0, .x))) corrplot(as_matrix(corr_high), type = &#39;lower&#39;, method = &#39;color&#39;, addgrid.col = &#39;aliceblue&#39;, tl.cex = 0.5) 2.3 Imputation Here I use MICE for imputation, setting my imputation model to a random forest model at the within-subject levels. We include in this model all predictors, and we set outcomes and ID to 0 so we can exclude them from any predictions. We then check if any variables have a constant value. This occurs for example in some of the substance use items, or perhaps location items. So we fill those with the constant value the participants endorsed (or didn’t). Once that’s done, we exclude those predictors from our matrix as well, and then implement MICE. # Select non-composite EMA variable ema_vars = c(ema_mor_vars, ema_day_vars, ema_eve_vars, categorical_items_d) # Now also select som EPA variables (subset sleep) epa_vars = c(epa_day_hr, epa_day_stress, epa_sleep_vars, epa_day_act[c(2:5,7,8)], epa_day_bat) # Vector of IVs of interest and n of features impute_vars = c(ema_vars, epa_vars) # Subset data df_features = df_merge %&gt;% select(external_id, week_nr, day_nr, prompt_num, phq9_sum_w, phq2_sum_e, all_of(impute_vars) ) %&gt;% filter(prompt_num != 5) run_imputation=FALSE if (run_imputation){ # Define the path for your error log error_log_file &lt;- paste0(&quot;imputation_failures_&quot;, Sys.Date(), &quot;.log&quot;) if (file.exists(error_log_file)) file.remove(error_log_file) # Progress bar set-up library(progress) pb &lt;- progress_bar$new(format = &quot;[:bar] :percent ETA: :eta&quot;, total = length(unique(df_features$external_id))) progress &lt;- function(n){ pb$tick()} opts &lt;- list(progress = progress) # Parallel set-up n_cores = detectCores()-1 cl = makeCluster(n_cores) registerDoSNOW(cl) # Parallel Run list_imputation_results = foreach(i = unique(df_features$external_id), .packages = c(&#39;mice&#39;,&#39;beepr&#39;, &quot;dplyr&quot;), .errorhandling = &#39;pass&#39;, .options.snow = opts, .verbose = F) %dopar% { # Run function and log failures tryCatch({ impute_subject(df_features, i, impute_vars )}, # On errror: error = function(e) { # Create error message failed_id &lt;- i error_message &lt;- paste0( &quot;Timestamp: &quot;, Sys.time(), &quot;, Subject_ID: &quot;, failed_id, &quot;, Error: &quot;, e$message, &quot;\\n&quot;) # Append the error message to the log file cat(error_message, file = error_log_file, append = TRUE) # Return NULL for the failed iteration return(NULL) } ) } stopCluster(cl) # Combine results and make an imputation column df_features_imputed = rbindlist(list_imputation_results) df_features_imputed$imputed &lt;- 1 # Subset unimputed subs impute_fail_subs &lt;- setdiff(df_features$external_id, df_features_imputed$external_id) df_features_unimputed &lt;- df_features[(df_features$external_id %in% impute_fail_subs),] df_features_unimputed$imputed &lt;- 0 # Now put them togerther df_features_mix = rbind(df_features_imputed, df_features_unimputed, fill = TRUE) # # # Impute the ones that didnt work at the between-subject level L2_prediction_mat &lt;- make_pred_matrix(df_features_mix, impute_vars = impute_vars)$pred_matrix L2_prediction_mat[, &quot;external_id&quot;] &lt;- -2 mice_imputation_2l &lt;- mice(data = df_features_mix, maxit = 2, method = &quot;2lonly.pmm&quot;, seed = 666, blocks = as.list(rownames(L2_prediction_mat)), predictorMatrix = L2_prediction_mat) df_features_mix &lt;- complete(mice_imputation_2l) # Make a day-prompt interaction for later df_features_mix$day_prompt = as.numeric(interaction(df_features_mix$day_nr, df_features_mix$prompt_num)) # Finally save the imputed data frame fwrite(df_features_mix, file = here(&#39;data&#39;, &#39;df_features_imputed.csv&#39;)) # Now separate the training and test sets df_features_mix = df_features_mix %&gt;% arrange(external_id, week_nr, day_nr, imputed) df_ml_train = df_features_mix %&gt;% filter(external_id %in% training_set$external_id) df_ml_test = df_features_mix %&gt;% filter(!(external_id %in% training_set$external_id)) # Write the data fwrite(df_ml_train, file = here(&quot;data&quot;, &quot;imputed_training.csv&quot;)) fwrite(df_ml_test, file = here(&quot;data&quot;, &quot;imputed_testing.csv&quot;)) }else{ df_features_mix &lt;- fread(file = here(&#39;data&#39;, &#39;df_features_imputed.csv&#39;)) df_ml_train = fread(file = here(&quot;data&quot;, &quot;imputed_training.csv&quot;)) df_ml_test = fread(file = here(&quot;data&quot;, &quot;imputed_testing.csv&quot;)) } 2.4 Dataset Creation We have completed the first part of data exploration and preparation and are now ready to start running our actual models. My aim is to first see if I can predict depression at the daily (PHQ-2) and weekly (PHQ-9) levels. I also want to see whether we can do this using EMA measures, or wearable measures. To get there, I use the Leiden University ALICE HPC cluster. The analysis is also done in python, so here I will prepare the data to send to the cluster as python pickles which I can then read into the cluster much more easily. Part of the preparation at this stage is to also standardize the variables within-subject. I then pivot the data to the required format. 2.4.1 Weekly Data # Pivot to wide and make matrix per variable features = impute_vars n_features = length(impute_vars) ## First for PA df_ml_lstm_week = df_features_mix %&gt;% select(external_id, week_nr, prompt_num, phq9_sum_w, day_nr, all_of(features)) %&gt;% filter(!is.na(day_nr)) %&gt;% # Now we back-fill the PHQ9 items again (cause we have to) group_by(external_id, week_nr) %&gt;% fill(phq9_sum_w, .direction = &#39;up&#39;) %&gt;% ungroup() %&gt;% # Now we rearrange and keep the distinct obesractions in case of dups arrange(external_id, week_nr, day_nr, prompt_num) %&gt;% distinct(external_id, week_nr, day_nr, prompt_num, .keep_all = TRUE) %&gt;% # Now create the PHQ-9 cut-off for classification mutate(phq9_cuttoff = ifelse(phq9_sum_w&gt;9, 1, 0)) %&gt;% # Scale and center non-binary feats group_by(external_id) %&gt;% mutate(across(all_of(c(ema_day_vars, epa_vars, ema_mor_vars, &quot;discomfort_e&quot;, &quot;emo_reg_e&quot;, &quot;useful_e&quot;, &quot;neg_exp_e&quot;,&quot;pos_exp_e&quot;)), ~ (.x - mean(.x, na.rm = T))/sd(.x, na.rm = T))) %&gt;% ungroup() %&gt;% # Pivot Wider pivot_wider(id_cols = c(external_id, week_nr, phq9_sum_w, phq9_cuttoff), names_from = c(day_nr, prompt_num), values_from = all_of(features)) %&gt;% select(!ends_with(&quot;_5&quot;)) %&gt;% na.omit() # Create training and test sets df_ml_lstm_week_train = df_ml_lstm_week[(df_ml_lstm_week$external_id %in% training_set$external_id),] df_ml_lstm_week_test = df_ml_lstm_week[!(df_ml_lstm_week$external_id %in% training_set$external_id),] 2.4.2 Daily Data # Pivot to wide and make matrix per variable features = impute_vars n_features = length(features) ## First for PA df_ml_lstm_day = df_features_mix %&gt;% select(external_id, week_nr, prompt_num, phq2_sum_e, day_nr, all_of(features)) %&gt;% filter(!is.na(day_nr)) %&gt;% # Now we back-fill the PHQ9 items again (cause we have to) group_by(external_id, week_nr, day_nr) %&gt;% fill(phq2_sum_e, .direction = &#39;up&#39;) %&gt;% ungroup() %&gt;% # Now we rearrange and keep the distinct obesractions in case of dups arrange(external_id, week_nr, day_nr, prompt_num) %&gt;% distinct(external_id, week_nr, day_nr, prompt_num, .keep_all = TRUE) %&gt;% # Scale and center group_by(external_id) %&gt;% mutate(across(all_of(features[-c(grep(&quot;cat&quot;, features), grep(&quot;substance&quot;, features))]), ~ (.x - mean(.x, na.rm = T))/sd(.x, na.rm = T))) %&gt;% ungroup() %&gt;% # Pivot Wider pivot_wider(id_cols = c(external_id, week_nr, day_nr, phq2_sum_e), names_from = c(prompt_num), values_from = features) %&gt;% select(!ends_with(&quot;_5&quot;)) %&gt;% na.omit() # Create training and test sets df_ml_lstm_day_train = df_ml_lstm_day[(df_ml_lstm_day$external_id %in% training_set$external_id),] df_ml_lstm_day_test = df_ml_lstm_day[!(df_ml_lstm_day$external_id %in% training_set$external_id),] 2.5 Dataset Export We now save the data as a python object since we run the final models there. We do this iteratively over the week/day and train/test data in a function format. This creates the following for each combination of the previously mentioned: *_sub: A file containing the subject rows for grouping purposes later. *_phq: A file with the Y outcomes *_ema: A file with the EMA features vars*: A file with the variable names for SHAP estimations and such library(reticulate) # Make sublists week_list &lt;- list( train = df_ml_lstm_week_train, val = df_ml_lstm_week_test ) day_list &lt;- list( train = df_ml_lstm_day_train, val = df_ml_lstm_day_test ) var_lists &lt;- list( ema = ema_vars, epa = epa_vars, all = c(ema_vars, epa_vars) ) pickle_jar = TRUE # TO avoid rewriting the OG data everytime we knit if (pickle_jar){ # Then inside the loop, you just need one line: for (timeframe in c(&quot;week&quot;, &quot;day&quot;)){ # Specify data to use if(timeframe == &quot;week&quot;){ tmp_list &lt;- week_list outcome = &#39;phq9_sum_w&#39; } else if(timeframe == &#39;day&#39;){ tmp_list &lt;- day_list outcome = &#39;phq2_sum_e&#39; } # Now do the train/test stuff for(train_test in c(&quot;train&quot;, &quot;val&quot;)){ tmp_data &lt;- tmp_list[[train_test]] ## Save subject IDs and outcomes tmp_data %&gt;% select(external_id) %&gt;% as.matrix() %&gt;% py_save_object(here(&#39;data&#39;, &#39;pickles&#39;, paste0(timeframe, &#39;_&#39;, train_test, &#39;_subs.pickle&#39;))) ## Outcome tmp_data %&gt;% select(all_of(outcome)) %&gt;% as.matrix() %&gt;% py_save_object(here(&#39;data&#39;, &#39;pickles&#39;, paste0(timeframe, &#39;_&#39;, train_test, &#39;_phq.pickle&#39;))) # Make the lagged-phq9 predictor tmp_data %&gt;% group_by(external_id) %&gt;% mutate(across(all_of(outcome), function(.x) { lagged_x &lt;- lag(.x) # Calculate median *once* to avoid re-computing median_val &lt;- round(median(na.omit(lagged_x))) lagged_x &lt;- ifelse(is.na(lagged_x), median_val, lagged_x) return(lagged_x) }, .names = &quot;{.col}_lag1&quot;)) %&gt;% ungroup() %&gt;% select(all_of(paste0(outcome, &quot;_lag1&quot;))) %&gt;% as.matrix() %&gt;% py_save_object(here(&#39;data&#39;, &#39;pickles&#39;, paste0(timeframe, &#39;_&#39;, train_test, &#39;_phq_lag1.pickle&#39;))) # Now we make the predictor matrices for (data in c(&quot;ema&quot;, &quot;epa&quot;, &quot;all&quot;)){ var_list &lt;- var_lists[[data]] feature_mat(tmp_data, vars = var_list) %&gt;% reticulate::r_to_py() %&gt;% py_save_object( here(&#39;data&#39;, &#39;pickles&#39;, paste0(timeframe, &#39;_&#39;, train_test, &#39;_&#39;, data, &#39;.pickle&#39;))) } } } } # And now we do the variable list separtely for (data in c(&quot;ema&quot;, &quot;epa&quot;, &quot;all&quot;)){ var_list &lt;- var_lists[[data]] var_list %&gt;% as.matrix() %&gt;% py_save_object(here(&#39;data&#39;, &#39;pickles&#39;, paste0(&#39;vars_&#39;, data, &#39;.pickle&#39;))) } "],["model-testing.html", "3 Model Testing 3.1 Model Predictions 3.2 SHAP Estimate", " 3 Model Testing Here is where we run the models on the test data. We do this in a loop across all timeframes/data sources. Importantly, this is done in python. I import os import pandas as pd import numpy as np # ML Libs from sklearn import preprocessing from sklearn import model_selection import tensorflow as tf from tensorflow import keras import shap # For plotting import matplotlib.pyplot as plt from sklearn.metrics import r2_score 3.1 Model Predictions # Define model parameters models = [&#39;week&#39;, &quot;day&quot;] sources = [&quot;ema&quot;, &quot;epa&quot;, &quot;all&quot;, &quot;phq_lag1&quot;] # Run loops for model_windows in models: for source in sources: print(f&#39;Running {model_windows} for {source} data...&#39;) # --- Load data --- try: # set data paths test_x_path = os.path.join(&#39;..&#39;, &#39;data&#39;, &#39;pickles&#39;, f&#39;{model_windows}_val_{source}.pickle&#39;) test_y_path = os.path.join(&#39;..&#39;,&#39;data&#39;, &#39;pickles&#39;, f&#39;{model_windows}_val_phq.pickle&#39;) test_subs_path = os.path.join(&#39;..&#39;,&#39;data&#39;, &#39;pickles&#39;, f&#39;{model_windows}_val_subs.pickle&#39;) # load data test_x = pd.read_pickle(test_x_path) test_y = np.float32(pd.read_pickle(test_y_path)) + 1 test_subs = pd.read_pickle(test_subs_path) except Exception as e: print(f&quot;Error loading data for model {model_windows}, source {source}: {e}&quot;) # --- Load models and predict --- all_predictions = pd.DataFrame() for i in range(1, 11): try: model_path = os.path.join(&#39;..&#39;,&#39;results&#39;, f&#39;model_{model_windows}_{source}_fold_{i}.keras&#39;) # Load the model model = tf.keras.models.load_model(model_path, compile=False) # Get predictions predicted_vals = model.predict(test_x) # Create a temporary DataFrame for this fold&#39;s results temp_preds = pd.DataFrame() temp_preds[&#39;predicted&#39;] = predicted_vals.flatten() temp_preds[&#39;actual&#39;] = test_y.flatten() temp_preds[&#39;model&#39;] = model_windows temp_preds[&#39;source&#39;] = source temp_preds[&#39;fold&#39;] = i temp_preds[&#39;sub_id&#39;] = test_subs.flatten() # Append the DataFrame to initial one all_predictions = pd.concat([all_predictions, temp_preds], ignore_index=True) except Exception as e: print(f&quot;Could not load/predict model for fold {i}, model {model_windows}, source {source}: {e}&quot;) ## Running week for ema data... ## Running week for epa data... ## Running week for all data... ## Running week for phq_lag1 data... ## Running day for ema data... ## Running day for epa data... ## Running day for all data... ## Running day for phq_lag1 data... # Save the predictions #all_predictions.to_csv(f&#39;../results/results_testing_{model_windows}_{source}.csv&#39;, index=False) 3.2 SHAP Estimate Here we estimated the SHAP values. Code is commented because it takes forever to run, and I don’t want to redo this everytime I am knitting the notebooks. # # Set up arrays and dfs for results # models = [&#39;day&#39;, &#39;week&#39;] # sources = [&#39;all&#39;, &#39;ema&#39;, &#39;epa&#39;] # # # # # Run loops # for model_windows in models: # for source in sources: # print(f&quot;Processing SHAP values for model: {model_windows}, source: {source}&quot;) # # Load data # train_x = pd.read_pickle(f&#39;../data/{model_windows}_train_{source}_new.pickle&#39;) # test_x = pd.read_pickle(f&#39;../data/{model_windows}_val_{source}_new.pickle&#39;) # test_y = np.float32(pd.read_pickle(f&#39;../data/{model_windows}_val_phq_new.pickle&#39;) + 1) # vars = pd.read_pickle(f&#39;../data/vars_{source}_new.pickle&#39;).flatten() # # # Load models # all_shaps = pd.DataFrame() # for i in range(1,11): # print(f&quot; Fold {i}&quot;) # # Fit the models and get the predictions # model = tf.keras.models.load_model(f&#39;../results/model_{model_windows}_{source}_fold_{i}.keras&#39;, compile=False) # predicted_vals = model.predict(test_x) # # # Run the shap explainer # background = shap.sample(train_x, 1000) # explainer = shap.GradientExplainer(model, background) # explanation = explainer(test_x) # explanation.feature_names = vars # # # Extract SHAP values # shap_values = explanation.values[:,:,:,0] # # Reshape the SHAP values to a 2D array # reshaped = shap_values.reshape(-1, shap_values.shape[-1]) # # Create sample and step indices # num_samples, num_steps, num_features = shap_values.shape # sample_idx = np.repeat(np.arange(num_samples), num_steps) # step_idx = np.tile(np.arange(num_steps), num_samples) # # # Build DataFrame # shap_df = pd.DataFrame(reshaped, columns=vars) # shap_df[&#39;sample&#39;] = sample_idx # shap_df[&#39;step&#39;] = step_idx # shap_df[&#39;fold&#39;] = i # shap_df[&#39;source&#39;] = source # shap_df[&#39;model&#39;] = model_windows # # # Append to master # all_shaps = pd.concat([all_shaps, shap_df], ignore_index=True) # # Save the SHAP values # all_shaps.to_csv(f&#39;../results/shap_values_{model_windows}_{source}.csv&#39;, index=False, ) "],["results.html", "4 Results 4.1 Model Performance 4.2 Idiographic Model Performance 4.3 SHAP 4.4 Supp Analysis", " 4 Results In this section, I create summary statistics and figures from the models we ran on the cluster. I first look at model performance statistics, look at idiographic estimates, and then make some plots for the paper. # Data wrangling library(data.table) library(tidyverse) # Stats library(lmerTest) library(emmeans) library(caret) # Plots library(ggplot2) library(ggtext) library(ggbeeswarm) library(gridExtra) # Fonts library(sysfonts) library(showtext) # Font stuffies # font_add(&quot;fa&quot;, &quot;figures/fa-regular-400.ttf&quot;) font_add(&quot;SegoeUIEmoji&quot;, &quot;C:/Windows/Fonts/seguiemj.ttf&quot;) font_add(&quot;SegoeSymb&quot;, &quot;C:/Windows/Fonts/seguisym.ttf&quot;) font_add(&quot;SegoeIcons&quot;, &quot;C:/Windows/Fonts/SegoeIcons.ttf&quot;) theme_update(text = element_text(family = &quot;SegoeIcons&quot;)) showtext_auto() # Functions confidence_interval &lt;- function(vector, interval) { # Standard deviation of sample vec_sd &lt;- sd(vector) # Sample size n &lt;- length(vector) # Mean of sample vec_mean &lt;- mean(vector) # Error according to t distribution error &lt;- qt((interval + 1)/2, df = n - 1) * vec_sd / sqrt(n) # Confidence interval as a vector result &lt;- c(&quot;lower&quot; = vec_mean - error, &quot;upper&quot; = vec_mean + error) return(result) } 4.1 Model Performance First check the model performance. I do this by loading the model results as csvs which contain the predictions and actual scores for each of the training and test set, then estimating the mean error (ME), mean absolute error (MAE), mean absolute percent error (MAPE), root mean square error (RMSE), and R^2. R^2 is our primary metric of interest here as its the easiest to interpret for psychologists (our target audience). 4.1.1 Training CV # Load the results from the training CV training_cv = Sys.glob(&quot;C:/Users/rayya/Nextcloud/FSW_WARN-D (Projectfolder)/group/raytut/projects/prediction_nomothetic/results/results_training*.csv&quot;) # Read all results into a df df_val = rbindlist(lapply(training_cv, fread)) df_val &lt;- df_val %&gt;% mutate(source = factor(source, levels = c(&#39;ema&#39;,&#39;epa&#39;, &#39;all&#39;, &quot;phq_lag1&quot;))) %&gt;% group_by(sub_id, fold, model, source) %&gt;% mutate(n = 1:length(actual)) df_val %&gt;% group_by(fold, model, source) %&gt;% mutate(R2 = R2(predicted, actual)) %&gt;% group_by(model, source) %&gt;% summarise(R2_sd = sd(R2, na.rm=T)) -&gt; train_r2sd ## `summarise()` has grouped output by &#39;model&#39;. You can override using the `.groups` argument. # Create a summary table df_val_summ &lt;- df_val %&gt;% group_by(model, source) %&gt;% arrange(model, source) %&gt;% mutate(predicted = round(predicted)) %&gt;% summarise(ME = (mean(predicted-actual)), ME_CI_L = confidence_interval((predicted-actual), 0.95)[1], ME_CI_U = confidence_interval((predicted-actual), 0.95)[2], MAE = mean(abs(predicted-actual)), MAPE = (mean(abs(predicted-actual)/actual))*100, RMSE = RMSE(predicted, actual), R2 = R2(predicted, actual)) %&gt;% mutate(across( c(ME, ME_CI_L, MAE, ME_CI_U, MAPE, RMSE, R2), ~round(.x, 2))) ## `summarise()` has grouped output by &#39;model&#39;. You can override using the `.groups` argument. df_val_summ &lt;- merge(df_val_summ, train_r2sd, by=c(&quot;model&quot;, &quot;source&quot;)) df_val_summ df_val_summ$set = &quot;cv&quot; 4.1.2 Test Data test_cv = Sys.glob(&quot;C:/Users/rayya/Nextcloud/FSW_WARN-D (Projectfolder)/group/raytut/projects/prediction_nomothetic/results/results_testing_*.csv&quot;) # Read all results into a df df_test = rbindlist(lapply(test_cv, fread), fill=T) df_test &lt;- df_test %&gt;% mutate(source = factor(source, levels = c(&#39;ema&#39;,&#39;epa&#39;, &#39;all&#39;, &quot;phq_lag1&quot;))) %&gt;% group_by(sub_id, fold, model, source) %&gt;% mutate(n = 1:length(actual)) %&gt;% ungroup() df_test %&gt;% group_by(fold, model, source) %&gt;% mutate(R2 = R2(predicted, actual)) %&gt;% ungroup() %&gt;% group_by(model, source) %&gt;% summarise(R2_sd = sd(R2, na.rm=T)) -&gt; testr2_sds ## `summarise()` has grouped output by &#39;model&#39;. You can override using the `.groups` argument. # Create a summary table df_test_summ &lt;- df_test %&gt;% group_by(model, source) %&gt;% arrange(model, source) %&gt;% mutate(predicted = round(predicted)) %&gt;% summarise(ME = (mean(predicted-actual)), ME_CI_L = confidence_interval((predicted-actual), 0.95)[1], ME_CI_U = confidence_interval((predicted-actual), 0.95)[2], MAE = mean(abs(predicted-actual)), MAPE = (mean(abs(predicted-actual)/actual))*100, RMSE = RMSE(predicted, actual), R2 = R2(predicted, actual)) %&gt;% mutate(across( c(ME, ME_CI_L, MAE, ME_CI_U, MAPE, RMSE, R2), ~round(.x, 2))) ## `summarise()` has grouped output by &#39;model&#39;. You can override using the `.groups` argument. # Add the folde R2 SDs and print summary df_test_summ &lt;- merge(df_test_summ, testr2_sds, by=c(&quot;model&quot;,&quot;source&quot;)) df_test_summ df_test_summ$set = &quot;test&quot; 4.1.3 Full Results df_results &lt;- rbind(df_val_summ, df_test_summ) df_results %&gt;% arrange(model, source, set) %&gt;% select(1,2,11, 3:9, 10) 4.2 Idiographic Model Performance I also estimate the idiographic model performance, just doing R^2 here. df_val$samp &lt;- &quot;10-fold Validation&quot; df_test$samp &lt;- &quot;Test Data&quot; df_idio &lt;- rbind(df_val, df_test) df_idio &lt;- df_idio %&gt;% group_by(sub_id, model, source, samp) %&gt;% mutate(R2 = R2(predicted, actual)) %&gt;% distinct(sub_id, .keep_all=T) ## Warning: There were 16 warnings in `mutate()`. ## The first warning was: ## ℹ In argument: `R2 = R2(predicted, actual)`. ## ℹ In group 5271: `sub_id = &quot;s665990513&quot;`, `model = &quot;week&quot;`, `source = ema`, `samp = &quot;Test Data&quot;`. ## Caused by warning in `cor()`: ## ! the standard deviation is zero ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 15 remaining warnings. ggplot(df_idio, aes(x=R2, fill=samp)) + geom_bar(breaks=seq(0, 1, by=0.1)) + theme_minimal() + facet_grid(model ~ source) ## Warning in geom_bar(breaks = seq(0, 1, by = 0.1)): Ignoring unknown parameters: `breaks` ## Warning: Removed 16 rows containing non-finite outside the scale range (`stat_count()`). df_idio_sum &lt;- Rmisc::summarySEwithin(df_idio, measurevar = &quot;R2&quot;, withinvars = c(&quot;model&quot;, &quot;source&quot;, &quot;samp&quot;), na.rm = T) ## Automatically converting the following non-factors to factors: model, samp ggplot(df_idio_sum, aes(y=R2, x=source, fill=samp, color=samp)) + geom_bar(stat=&quot;summary&quot;, position=position_dodge2(), alpha=0.5) + geom_point(stat=&quot;summary&quot;, position=position_dodge(0.9)) + geom_errorbar(aes(ymin=R2-ci, ymax = R2+ci), position = position_dodge2()) + theme_minimal() + theme(axis.title.x = element_blank(), legend.title = element_blank()) + facet_grid(model~.) ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` 4.3 SHAP Now I estimate the SHAP plots for feature contributions. I wrap this into a function that lets me plot a bit more easily and keep some visual consistency. The funciton will filter the data to reduce overhead memory usage, rank the SHAP values based on the absolute SHAP values, and then filter and plot the top 10 features. I do the data loading after setting-up the function to read all the SHAP values. # Plotting functions shap_plot &lt;- function(shap_values, models=&#39;week&#39;, sources=&#39;ema&#39;, feats = 10, x_range=0.5){ # Filter and estimate abs values df_temp &lt;- shap_values[model == models &amp; source == sources] df_temp[, shap_mean := mean(abs(shap_fold_avg)), by = feature] df_temp[, shap_order := frank(-abs(shap_mean), ties.method = &quot;dense&quot;)] df_temp[, shap_order := shap_order-1] # Keep top features df_temp_top &lt;- df_temp[shap_order &lt;= feats-1] df_temp_top$feature_labs &lt;- droplevels(df_temp_top$feature_labs) df_temp_top$feature_labs &lt;- fct_rev(factor(df_temp_top$feature_labs, levels = unique(df_temp_top[order(shap_order)]$feature_labs))) # Plot plot = ggplot(df_temp_top, aes(x=shap_fold_avg, y = feature_labs)) + geom_vline(xintercept = 0) + geom_quasirandom(aes(color = shap_fold_avg), alpha=0.2) + scale_y_discrete( limits = levels(df_temp_top$feature_labs), labels = levels(df_temp_top$feature_labs)) + scale_x_continuous(breaks = seq(-1, 1, by=0.5)) + scale_color_gradient2(low = &quot;blue&quot;, mid=&#39;yellow&#39;, high = &quot;red&quot;, midpoint = 0) + labs(y = &quot;&quot;, x = &quot;SHAP Values&quot;) + theme_light() + theme( plot.title=element_text(face = &quot;bold&quot;, hjust = 0.5, size=7), legend.position=&#39;none&#39;, axis.text.y = element_text(family = &quot;SegoeSymb&quot;, size=6.5, angle=45, margin = margin(l=25)), axis.title.x = element_text(size=7, margin = margin(t=10, b=10)), axis.text.x = element_text( size=6.5)) + coord_cartesian(xlim=c(-x_range,x_range)) return(plot) } shap_plot_ts&lt;- function(shap_values, models, sources){ # Filter and estimate abs values df_temp &lt;- shap_values[model == models &amp; source == sources] df_temp[, shap_mean := mean(abs(shap_fold_avg)), by = feature] df_temp[, shap_order := frank(-abs(shap_mean), ties.method = &quot;dense&quot;)] df_temp[,step := step] # Keep top features df_temp_top &lt;- df_temp[shap_order &lt;= 10] df_temp_top$feature_labs &lt;- droplevels(df_temp_top$feature_labs) df_temp_top$feature_labs &lt;- factor(df_temp_top$feature_labs, levels = unique(df_temp_top[order(shap_order)]$feature_labs)) # Plot plot &lt;- ggplot(df_temp_top, aes(x=step, y = shap_fold_avg)) + geom_hline(yintercept = 0) + geom_quasirandom(aes(color = shap_fold_avg), size = 0.2) + scale_color_viridis_c() + labs(y = &quot;SHAP Values&quot;, x = &quot;Time Steps&quot;) + theme_light() + theme( plot.title=element_text(face = &quot;bold&quot;, hjust = 0.5), legend.position=&#39;none&#39;, axis.text = element_text(size=18), axis.title = element_text(size=21), strip.text = element_text(family = &quot;SegoeSymb&quot;, size=21)) + facet_wrap(feature_labs~., nrow=2, ncol=5) return(plot) } # Read in the Python Estimated SHAP values shap_values = Sys.glob(&quot;C:/Users/rayya/Nextcloud/FSW_WARN-D (Projectfolder)/group/raytut/projects/prediction_nomothetic/results/shap_values_*.csv&quot;) shap_values = rbindlist(lapply(shap_values, fread), fill=T) setDT(shap_values) # Convert to a long dataframe shap_values &lt;- melt(shap_values, id.vars = c(&quot;sample&quot;, &quot;step&quot;, &quot;fold&quot;, &quot;source&quot;, &quot;model&quot;), variable.name = &quot;feature&quot;, value.name = &quot;shap&quot;) # Average across folds shap_values[, shap_fold_avg := mean(shap, na.rm=T), by = c(&#39;sample&#39;, &#39;step&#39;, &#39;model&#39;, &#39;source&#39;, &#39;feature&#39;)] shap_values[, fold := NULL] shap_values[, shap := NULL] shap_values &lt;- unique(shap_values) # Make prettier labels variable_codes &lt;- c( &quot;sleep_qual_m&quot;, &quot;sleep_rest_m&quot;, &quot;outlook_m&quot;, &quot;overwhelm_d&quot;, &quot;motivated_d&quot;, &quot;stressed_d&quot;, &quot;ruminate_d&quot;, &quot;tired_d&quot;, &quot;nervous_d&quot;, &quot;sad_d&quot;, &quot;relaxed_d&quot;, &quot;cheerful_d&quot;, &quot;irritable_d&quot;, &quot;activity_enjoy_d&quot;, &quot;offline_enjoy_d&quot;, &quot;online_enjoy_d&quot;, &quot;discomfort_e&quot;, &quot;emo_reg_e&quot;, &quot;useful_e&quot;, &quot;neg_exp_e&quot;, &quot;pos_exp_e&quot;, &quot;neg_exp_cat_e_1&quot;, &quot;neg_exp_cat_e_2&quot;, &quot;neg_exp_cat_e_3&quot;, &quot;neg_exp_cat_e_10&quot;, &quot;neg_exp_cat_e_4&quot;, &quot;neg_exp_cat_e_5&quot;, &quot;neg_exp_cat_e_6&quot;, &quot;neg_exp_cat_e_11&quot;, &quot;neg_exp_cat_e_7&quot;, &quot;neg_exp_cat_e_8&quot;, &quot;neg_exp_cat_e_12&quot;, &quot;pos_exp_cat_e_1&quot;, &quot;pos_exp_cat_e_2&quot;, &quot;pos_exp_cat_e_3&quot;, &quot;pos_exp_cat_e_10&quot;, &quot;pos_exp_cat_e_4&quot;, &quot;pos_exp_cat_e_5&quot;, &quot;pos_exp_cat_e_6&quot;, &quot;pos_exp_cat_e_11&quot;, &quot;pos_exp_cat_e_7&quot;, &quot;pos_exp_cat_e_8&quot;, &quot;pos_exp_cat_e_12&quot;, &quot;substance_e_1&quot;, &quot;substance_e_2&quot;, &quot;substance_e_3&quot;, &quot;substance_e_4&quot;, &quot;substance_e_5&quot;, &quot;substance_e_6&quot;, &quot;substance_e_7&quot;, &quot;substance_e_8&quot;, &quot;substance_e_9&quot;, &quot;hr_mean&quot;, &quot;hr_min&quot;, &quot;hr_max&quot;, &quot;hr_sd&quot;, &quot;stress_mean&quot;, &quot;stress_sd&quot;, &quot;stress_min&quot;, &quot;stress_max&quot;, &quot;intraday_activity_cals&quot;, &quot;intraday_steps&quot;, &quot;intraday_active_seconds&quot;, &quot;intraday_met_value&quot;, &quot;intraday_intensity_mean&quot;, &quot;intraday_intensity_max&quot;, &quot;body_bat_mean&quot;, &quot;body_bat_sd&quot;, &quot;body_bat_min&quot;, &quot;body_bat_max&quot;, &quot;sleep_hr_mean&quot;, &quot;sleep_hr_median&quot;, &quot;sleep_hr_skew&quot;, &quot;sleep_hr_kurt&quot;, &quot;sleep_hr_min&quot;, &quot;sleep_hr_max&quot;, &quot;sleep_hr_sd&quot;, &quot;sleep_start&quot;, &quot;sleep_end&quot;, &quot;sleep_dur_tot_s&quot; ) # Have to them one line at a time unfortsh variable_labels &lt;- c( &quot;Sleep Quality&quot;,&quot;Rested&quot;, &quot;Outlook&quot;, &quot;☀Overwhelm&quot;, &quot;☀Motivated&quot;, &quot;☀Stressed&quot;, &quot;☀Ruminate&quot;, &quot;☀Tired&quot;,&quot;☀Nervous&quot;, &quot;☀Sad&quot;, &quot;☀Relaxed&quot;, &quot;☀Cheerful&quot;, &quot;☀Irritable&quot;, &quot;☀Activity&quot;, &quot;☀Social (offline)&quot;, &quot;☀Social (online)&quot;, &quot;Discomfort&quot;, &quot;Emo. Regulation&quot;, &quot;Useful&quot;, &quot;Neg. Experience&quot;, &quot;Pos. Experience&quot;, &quot;Neg. Wellbeing (Phys)&quot;, &quot;Neg. Wellbeing (Ment)&quot;, &quot;Neg. Finances&quot;, &quot;Neg. Education/Work&quot;, &quot;Neg. Home&quot;, &quot;Neg. Leisure&quot;, &quot;Neg. Love Life&quot;, &quot;Neg. Social Life&quot;, &quot;Neg. Relationships (Family)&quot;, &quot;Neg. Experiences (Friends/Family)&quot;, &quot;Neg. Society/Politics&quot;, &quot;Pos. Wellbeing (Phys)&quot;, &quot;Pos. Wellbeing (Ment)&quot;, &quot;Neg. Finances&quot;, &quot;Pos. Education/Work&quot;, &quot;Pos. Home&quot;, &quot;Pos. Leisure&quot;, &quot;Pos. Love Life&quot;, &quot;Pos. Social Life&quot;, &quot;Pos. Relationships (Family)&quot;, &quot;Pos. Experiences (Friends/Family)&quot;, &quot;Pos. Society/Politics&quot;, &quot;Caffeine&quot;, &quot;Tobacco&quot;, &quot;Alcohol&quot;, &quot;Cannabis&quot;, &quot;Ecstasy&quot;, &quot;Amphetamines&quot;, &quot;Cocaine&quot;, &quot; Other Subs.&quot;, &quot;No Subs.&quot;, &quot;️☀HR Mean⌚&quot;, &quot;☀HR Min⌚&quot;, &quot;☀HR Max⌚️&quot;, &quot;☀HR SD⌚️&quot;, &quot;☀Stress Mean⌚&quot;, &quot;☀Stress SD⌚&quot;, &quot;☀Stress Min⌚&quot;, &quot;☀Stress Max⌚️&quot;, &quot;☀Calories⌚&quot;, &quot;☀Steps⌚&quot;, &quot;☀Active Time (s)⌚&quot;, &quot;☀MET Value⌚&quot;, &quot;☀Activity Intensity⌚&quot;, &quot;☀Maximum Intensity⌚&quot;, &quot;☀Body Bat. Mean⌚️&quot;, &quot;☀Body Bat. SD⌚&quot;, &quot;☀Body Bat. Min⌚&quot;, &quot;☀Body Bat. Max⌚&quot;, &quot;Sleep HR Mean⌚️&quot;, &quot;Sleep HR Median⌚️&quot;, &quot;Sleep HR Skew⌚️&quot;, &quot;Sleep HR Kurtosis⌚️&quot;, &quot;Sleep HR Min⌚️&quot;, &quot;Sleep HR Max⌚️&quot;, &quot;Sleep HR SD⌚️&quot;, &quot;Sleep Start⌚️&quot;, &quot;Sleep End⌚&quot;, &quot;Sleep Duration⌚️&quot;) # Make a label column for parsing shap_values$feature_labs &lt;- factor(shap_values$feature, levels=variable_codes, labels=variable_labels) 4.3.1 Top 10 Daily # Set up the three plots plot_d1 &lt;- shap_plot(shap_values, &quot;day&quot;, &quot;ema&quot;, 10, 0.8) + labs(title = &quot;Smartphone Self-Report (EMA)&quot;) plot_d2 &lt;- shap_plot(shap_values, &quot;day&quot;, &quot;epa&quot;, 10, 0.8) + labs(title = &quot;Smartwatch Passive Sensing&quot;) plot_d3 &lt;- shap_plot(shap_values, &quot;day&quot;, &quot;all&quot;, 10, 0.8) + labs(title = &quot;Combined Model&quot;) # Arrange them and save em day_plot &lt;- grid.arrange(plot_d1, plot_d2, plot_d3, ncol = 3) ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ggsave(&quot;../figures/figure_1_daily_mods.svg&quot;, plot=day_plot, dpi=600, width=18, height=12, units=&quot;cm&quot;, device=svglite::svglite) ggsave(&quot;../figures/figure_1_daily_mods.png&quot;, plot=day_plot, dpi=600, width=18, height=12, units=&quot;cm&quot;, device=ragg::agg_png) Then we check the SHAP values per timestep. # Plot plot_d_3_ts &lt;- shap_plot_ts(shap_values, &quot;day&quot;, &quot;all&quot;) plot_d_3_ts ggsave(&quot;../figures/sm_figure_daily_ts_shap.svg&quot;, plot_d_3_ts, dpi=300, width=18, height=14, units=&quot;cm&quot;,device=svglite::svglite) ggsave(&quot;../figures/sm_figure_daily_ts_shap.png&quot;, plot_d_3_ts, dpi=900,width=18, height=14, units=&quot;cm&quot;,device=ragg::agg_png) 4.3.2 Top 10 Weekly Check the overall SHAP values # Set up the three plots plot_w1 &lt;- shap_plot(shap_values, &quot;week&quot;, &quot;ema&quot;, 10, 0.8) + labs(title = &quot;Smartphone Self-Report (EMA)&quot;) plot_w2 &lt;- shap_plot(shap_values, &quot;week&quot;, &quot;epa&quot;, 10, 0.8) + labs(title = &quot;Smartwatch Passive Sensing&quot;) plot_w3 &lt;- shap_plot(shap_values, &quot;week&quot;, &quot;all&quot;, 10, 0.8) + labs(title = &quot;Combined Model&quot;) # Arrange em and save em week_plot &lt;- grid.arrange(plot_w1, plot_w2, plot_w3, ncol = 3) ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ## Orientation inferred to be along y-axis; override with `position_quasirandom(orientation = &#39;x&#39;)` ggsave(&quot;../figures/figure_2_weekly_mods.svg&quot;, week_plot, dpi=600, width=18, height=12, units=&quot;cm&quot;, device=svglite::svglite) ggsave(&quot;../figures/figure_2_weekly_mods.png&quot;, week_plot, dpi=600, width=18, height=12, units=&quot;cm&quot;, device=ragg::agg_png) Now check the SHAP Values per timestep. # Plot plot_w_3_ts &lt;- shap_plot_ts(shap_values, &quot;week&quot;, &quot;all&quot;) plot_w_3_ts ggsave(&quot;../figures/sm_figure_weekly_ts_shap.svg&quot;, plot_w_3_ts, dpi=300, width=18, height=14, units=&quot;cm&quot;, device=svglite::svglite) ggsave(&quot;../figures/sm_figure_weekly_ts_shap.png&quot;, plot_w_3_ts, dpi=900, width=18, height=14, units=&quot;cm&quot;, device=ragg::agg_png) 4.4 Supp Analysis 4.4.1 PHQ-9 Autoregressive Model We investigate the PHQ-9 model performance visually to see whats going on in the weekly models. df_val_phq_lead &lt;- df_test %&gt;% filter( model == &#39;week&#39; &amp; source == &quot;phq_lag1&quot;) %&gt;% group_by(sub_id, fold) %&gt;% mutate(phq_mean = mean(actual, na.rm=T)) %&gt;% mutate(predicted_lead = lead(predicted,n = 1)) %&gt;% select(sub_id, model,n, source, actual, predicted_lead, phq_mean) %&gt;% rename(predicted = predicted_lead) %&gt;% mutate(source = &quot;phq_lead&quot;) ## Adding missing grouping variables: `fold` df_val_phq_combi &lt;- df_test %&gt;% filter( model == &#39;week&#39; &amp; (source == &quot;all&quot; | source == &quot;phq_lag1&quot;)) %&gt;% group_by(sub_id, fold) %&gt;% mutate(phq_mean = mean(actual, na.rm=T)) %&gt;% select(sub_id, model, n, source, actual, predicted, phq_mean) ## Adding missing grouping variables: `fold` df_val_phq_combi &lt;- rbind(df_val_phq_combi, df_val_phq_lead) df_val_phq_combi &lt;- df_val_phq_combi %&gt;% ungroup() %&gt;% pivot_longer(cols=c(predicted, actual), values_to = &quot;phq&quot;, names_to = &quot;score&quot;) %&gt;% mutate(predictors = factor(source, levels=c(&quot;all&quot;, &quot;phq_lag1&quot;, &quot;phq_lead&quot;), labels=c(&quot;All&quot;, &quot;Lagged PHQ&quot;, &#39;&quot;Unlagged&quot; PHQ&#39;)), score = factor(score, levels = c(&quot;predicted&quot;, &quot;actual&quot;), labels=c(&quot;Predicted&quot;, &quot;Actual&quot;))) # Subset random subs set.seed(192) random_subs = sample(unique(df_val_phq_combi$sub_id),9) df_val_phq_combi &lt;- df_val_phq_combi[df_val_phq_combi$sub_id %in% random_subs,] # Plot ggplot(df_val_phq_combi, aes(x=n, y=phq, color=score) ) + geom_line(stat = &#39;summary&#39;) + geom_hline(aes(yintercept=phq_mean), linetype=&#39;dashed&#39;) + facet_grid(sub_id~predictors) + labs(y = &quot;PHQ-9 sum score&quot;, x =&quot;Time steps&quot;) + theme_light() + theme(legend.position=&quot;bottom&quot;, legend.title =element_blank(), text = element_text(size=32), strip.text = element_text(size=22)) ## Warning: Removed 90 rows containing non-finite outside the scale range (`stat_summary()`). ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ggsave(&quot;../figures/sm_figure_1_weekly_unlagged.svg&quot;, dpi=300, width=18, height=22, units=&quot;cm&quot;, device=svglite::svglite) ## Warning: Removed 90 rows containing non-finite outside the scale range (`stat_summary()`). ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ggsave(&quot;../figures/sm_figure_1_weekly_unlagged.png&quot;, dpi=300, width=18, height=22, units=&quot;cm&quot;, device=ragg::agg_png) ## Warning: Removed 90 rows containing non-finite outside the scale range (`stat_summary()`). ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
